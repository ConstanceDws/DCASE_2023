{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ConstanceDws/DCASE_2023/blob/main/DCASE23_Carbon_Metrics_Turorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diHH0973kekr"
      },
      "source": [
        "# ðŸŒ± Monitoring environmental impact of DCASE systems : Hands-On Tutorial â–¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYFiQOcAtLav"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "%%capture\n",
        "!pip install torch==1.13.1\n",
        "!pip install torchaudio==0.13.1\n",
        "!pip install codecarbon\n",
        "!pip install carbontracker\n",
        "!pip install pyJoules\n",
        "!pip install thop\n",
        "!pip install deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWlMeuLVvKt0",
        "outputId": "62144163-82ea-4b49-cb2d-b3a0fa63a737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 20 07:49:56 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    32W /  70W |   9065MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU configuration.\n",
        "# If you get an error, check if the running device by going to Runtime -> Change Runtime type -> T4 GPU.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2clUn6aGin8"
      },
      "source": [
        "#Before going any further, get access to the dataset and drag and drop it in your Drive :\n",
        "\n",
        "# https://drive.google.com/drive/folders/1hEzIWq3F-ycoEMcBxkzmJlqbGLWrQz21?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W8bKKUVGbZt"
      },
      "outputs": [],
      "source": [
        "# All needed imports\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I-iHbfBFRdf",
        "outputId": "b3e54d59-b05b-412a-9b6a-480e572265be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to access your data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC7X5J3bF2w8"
      },
      "outputs": [],
      "source": [
        "# Specify the path to your Google Drive folder containing audio files\n",
        "google_drive_path = '/content/drive/My Drive/DCASE23_Tutorial/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4KGc82KF8yG",
        "outputId": "1f15b755-a57f-49e9-8694-2c62de523cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data folder: /content/drive/My Drive/DCASE23_Tutorial/DESED_public_eval_sample\n",
            "data folder: /content/drive/My Drive/DCASE23_Tutorial/public_sample.tsv\n"
          ]
        }
      ],
      "source": [
        "# Create a dataset with the specified data folder\n",
        "data_folder = os.path.join(google_drive_path, 'DESED_public_eval_sample')\n",
        "metadata_file = os.path.join(google_drive_path, 'public_sample.tsv')\n",
        "print(f\"data folder: {data_folder}\")\n",
        "print(f\"data folder: {metadata_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIUdEhmJ-hBi"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "# Define the label-to-number mapping dictionary\n",
        "classes_labels = OrderedDict(\n",
        "    {\n",
        "        \"Alarm_bell_ringing\": 0,\n",
        "        \"Blender\": 1,\n",
        "        \"Cat\": 2,\n",
        "        \"Dishes\": 3,\n",
        "        \"Dog\": 4,\n",
        "        \"Electric_shaver_toothbrush\": 5,\n",
        "        \"Frying\": 6,\n",
        "        \"Running_water\": 7,\n",
        "        \"Speech\": 8,\n",
        "        \"Vacuum_cleaner\": 9,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Create a data loader\n",
        "batch_size = 32\n",
        "fs = 44100\n",
        "\n",
        "target_len = 10 * fs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhxwUgerG3oN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize an empty metadata dictionary\n",
        "metadata = {}\n",
        "\n",
        "# Read the metadata file (weak.tsv) while skipping the first line (headers)\n",
        "with open(metadata_file, 'r') as file:\n",
        "    next(file)  # Skip the first line (headers)\n",
        "    for line in file:\n",
        "        parts = line.strip().split()\n",
        "        filename = parts[0]\n",
        "\n",
        "        # Extract all event labels from the line\n",
        "        event_labels = parts[1].split(',')\n",
        "\n",
        "        # Map each event label to a number and store in a list\n",
        "        encoded_event_labels = [classes_labels.get(label, -1) for label in event_labels]\n",
        "\n",
        "        # Add the filename and encoded event labels to the metadata dictionary\n",
        "        metadata[filename] = encoded_event_labels\n",
        "\n",
        "# Create the dataset with metadata and transform - need to change this\n",
        "#transform = T.MFCC(sample_rate=16000, n_mfcc=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnSu-10EBc6L"
      },
      "outputs": [],
      "source": [
        "# padding the audio\n",
        "def pad_audio(waveform):\n",
        "\n",
        "  if waveform.shape[-1] < target_len:\n",
        "    waveform = torch.nn.functional.pad(\n",
        "        waveform, (0, target_len - waveform.shape[-1]), mode=\"constant\")\n",
        "\n",
        "  elif len(waveform) > target_len:\n",
        "    rand_onset = random.randint(0, len(waveform) - target_len)\n",
        "    waveform = waveform[rand_onset:rand_onset + target_len]\n",
        "\n",
        "  return waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfYQvmfFBks3"
      },
      "outputs": [],
      "source": [
        "def to_mono(waveform):\n",
        "  if waveform.shape[0] > 1:\n",
        "    indx = np.random.randint(0, waveform.shape[0] - 1)\n",
        "    waveform = waveform[indx]\n",
        "    waveform = waveform.unsqueeze(0)\n",
        "  return waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP3d3CTqLCU8"
      },
      "outputs": [],
      "source": [
        "from torchaudio.transforms import MelSpectrogram\n",
        "\n",
        "class WeakDataset(Dataset):\n",
        "    def __init__(self, data_folder, metadata, target_len, transform=None):\n",
        "        self.data_folder = data_folder\n",
        "        self.metadata = metadata\n",
        "        self.file_list = os.listdir(data_folder)\n",
        "        self.transform = MelSpectrogram(\n",
        "            sample_rate=44100,\n",
        "            n_fft=2048,\n",
        "            win_length=2048,\n",
        "            hop_length=256,\n",
        "            f_min=0,\n",
        "            f_max=22050,\n",
        "            n_mels=128,\n",
        "            window_fn=torch.hamming_window,\n",
        "            wkwargs={\"periodic\": False},\n",
        "            power=1,\n",
        "        )\n",
        "        self.target_len = target_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.file_list[idx]\n",
        "\n",
        "        file_path = os.path.join(self.data_folder, filename)\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "        # only one channel\n",
        "        waveform = to_mono(waveform)\n",
        "\n",
        "        # pad audio\n",
        "        waveform = pad_audio(waveform)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "\n",
        "        event_label = self.metadata[filename]\n",
        "\n",
        "        return waveform, event_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0k5NOShL6Ng"
      },
      "outputs": [],
      "source": [
        "#Define the CRNN model\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CRNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.rnn = nn.GRU(input_size=128, hidden_size=64, num_layers=1, batch_first=True)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.max_pool2d(x, kernel_size=(2, 2))\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.max_pool2d(x, kernel_size=(2, 2))\n",
        "        x = x.view(x.size(0), -1, x.size(1))\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PspzFeUjMHgv"
      },
      "outputs": [],
      "source": [
        "dataset = WeakDataset(data_folder, metadata, target_len)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgpMWbbJQKUr"
      },
      "source": [
        "## Use of [Deepspeed](https://www.deepspeed.ai/tutorials/flops-profiler/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZMwvI44ICiA",
        "outputId": "45ef2fe9-853a-4533-f4f1-d0bd739d04e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-09-20 07:50:02,282] [INFO] [profiler.py:1205:get_model_profile] Flops profiler warming-up...\n",
            "[2023-09-20 07:50:03,731] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
            "\n",
            "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
            "Profile Summary at step 1:\n",
            "Notations:\n",
            "data parallel size (dp_size), model parallel size(mp_size),\n",
            "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
            "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
            "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
            "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
            "\n",
            "params per GPU:                                                         112.39 K\n",
            "params of model = params per GPU * mp_size:                             0       \n",
            "fwd MACs per GPU:                                                       4.19 GMACs\n",
            "fwd flops per GPU:                                                      9.42 G  \n",
            "fwd flops of model = fwd flops per GPU * mp_size:                       9.42 G  \n",
            "fwd latency:                                                            1.21 s  \n",
            "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    7.76 GFLOPS\n",
            "\n",
            "----------------------------- Aggregated Profile per GPU -----------------------------\n",
            "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
            "depth 0:\n",
            "    params      - {'CRNN': '112.39 K'}\n",
            "    MACs        - {'CRNN': '4.19 GMACs'}\n",
            "    fwd latency - {'CRNN': '1.21 s'}\n",
            "\n",
            "------------------------------ Detailed Profile per GPU ------------------------------\n",
            "Each module profile is listed after its name in the following order: \n",
            "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
            "\n",
            "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
            "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
            "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
            "\n",
            "CRNN(\n",
            "  112.39 K = 100% Params, 4.19 GMACs = 100% MACs, 1.21 s = 100% latency, 7.76 GFLOPS\n",
            "  (conv1): Conv2d(640 = 0.57% Params, 127.03 MMACs = 3.03% MACs, 64.65 ms = 5.32% latency, 4.15 GFLOPS, 1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(73.86 K = 65.71% Params, 4.06 GMACs = 96.97% MACs, 135.24 ms = 11.14% latency, 60.13 GFLOPS, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (rnn): GRU(37.25 K = 33.14% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 128, 64, batch_first=True)\n",
            "  (fc): Linear(650 = 0.58% Params, 640 MACs = 0% MACs, 101.8 us = 0.01% latency, 12.57 MFLOPS, in_features=64, out_features=10, bias=True)\n",
            "  (softmax): Softmax(0 = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0% latency, 187.25 KFLOPS, dim=1)\n",
            ")\n",
            "------------------------------------------------------------------------------\n",
            "[2023-09-20 07:50:05,122] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from deepspeed.profiling.flops_profiler import get_model_profile\n",
        "\n",
        "model = CRNN(num_classes=10)\n",
        "shape = (1, 1, 128, 1723)\n",
        "\n",
        "flops, macs, params = get_model_profile(model=model, input_shape=(shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wZVyInTkGCS"
      },
      "source": [
        "## Use of [THOP](https://pypi.org/project/thop/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4-tkmC1IFmo",
        "outputId": "520f275c-d795-4106-c0ba-8d074409fe3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "MACS: 4.708G PARAMS:112.394K\n"
          ]
        }
      ],
      "source": [
        "from thop import profile, clever_format\n",
        "\n",
        "input = torch.randn(shape)\n",
        "\n",
        "macs, params = profile(model, inputs=(input,))\n",
        "\n",
        "macs, params = clever_format([macs, params], \"%.3f\")\n",
        "print(f\"MACS: {macs} PARAMS:{params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQhE08l2A0rQ"
      },
      "source": [
        "## Use [CodeCarbon](https://github.com/mlco2/codecarbon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aRqE3uQ4oXo",
        "outputId": "5463d559-5ca1-4936-b2de-ef65c7a4579f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 07:50:05] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 07:50:05] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 07:50:05] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 07:50:06] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 07:50:06] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon WARNING @ 07:50:07] We saw that you have a Intel(R) Xeon(R) CPU @ 2.30GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 07:50:07] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "[codecarbon INFO @ 07:50:07] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 07:50:07]   Platform system: Linux-5.15.109+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 07:50:07]   Python version: 3.10.12\n",
            "[codecarbon INFO @ 07:50:07]   CodeCarbon version: 2.3.1\n",
            "[codecarbon INFO @ 07:50:07]   Available RAM : 12.678 GB\n",
            "[codecarbon INFO @ 07:50:07]   CPU count: 2\n",
            "[codecarbon INFO @ 07:50:07]   CPU model: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "[codecarbon INFO @ 07:50:07]   GPU count: 1\n",
            "[codecarbon INFO @ 07:50:07]   GPU model: 1 x Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 2.301406443119049\n",
            "Epoch 2/20, Loss: 2.2891581058502197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 07:50:23] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.754403591156006 W\n",
            "[codecarbon INFO @ 07:50:23] Energy consumed for all GPUs : 0.000205 kWh. Total GPU Power : 49.07669187819857 W\n",
            "[codecarbon INFO @ 07:50:23] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 07:50:23] 0.000402 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20, Loss: 2.276199698448181\n",
            "Epoch 4/20, Loss: 2.274015963077545\n",
            "Epoch 5/20, Loss: 2.225351095199585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 07:50:38] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.754403591156006 W\n",
            "[codecarbon INFO @ 07:50:38] Energy consumed for all GPUs : 0.000419 kWh. Total GPU Power : 51.5110830658931 W\n",
            "[codecarbon INFO @ 07:50:38] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 07:50:38] 0.000813 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20, Loss: 2.2511150240898132\n",
            "Epoch 7/20, Loss: 2.2589438557624817\n",
            "Epoch 8/20, Loss: 2.2519461512565613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 07:50:53] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.754403591156006 W\n",
            "[codecarbon INFO @ 07:50:53] Energy consumed for all GPUs : 0.000634 kWh. Total GPU Power : 51.766035076577694 W\n",
            "[codecarbon INFO @ 07:50:53] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 07:50:53] 0.001225 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20, Loss: 2.205993413925171\n",
            "Epoch 10/20, Loss: 2.2528743147850037\n",
            "Epoch 11/20, Loss: 2.2404876351356506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 07:51:08] Energy consumed for RAM : 0.000079 kWh. RAM Power : 4.754403591156006 W\n",
            "[codecarbon INFO @ 07:51:08] Energy consumed for all GPUs : 0.000849 kWh. Total GPU Power : 51.63764676647107 W\n",
            "[codecarbon INFO @ 07:51:08] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 07:51:08] 0.001637 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20, Loss: 2.226578950881958\n",
            "Epoch 13/20, Loss: 2.2285507321357727\n",
            "Epoch 14/20, Loss: 2.1439427733421326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 07:51:23] Energy consumed for RAM : 0.000099 kWh. RAM Power : 4.754403591156006 W\n",
            "[codecarbon INFO @ 07:51:23] Energy consumed for all GPUs : 0.001062 kWh. Total GPU Power : 50.97124102838247 W\n",
            "[codecarbon INFO @ 07:51:23] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 07:51:23] 0.002047 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20, Loss: 2.200872004032135\n",
            "Epoch 16/20, Loss: 2.201626479625702\n",
            "Epoch 17/20, Loss: 2.2519859671592712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 07:51:38] Energy consumed for RAM : 0.000119 kWh. RAM Power : 4.754403591156006 W\n",
            "[codecarbon INFO @ 07:51:38] Energy consumed for all GPUs : 0.001275 kWh. Total GPU Power : 51.171948736857956 W\n",
            "[codecarbon INFO @ 07:51:38] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 07:51:38] 0.002457 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20, Loss: 2.247547686100006\n",
            "Epoch 19/20, Loss: 2.1497440338134766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 07:51:48] Energy consumed for RAM : 0.000133 kWh. RAM Power : 4.754403591156006 W\n",
            "[codecarbon INFO @ 07:51:48] Energy consumed for all GPUs : 0.001429 kWh. Total GPU Power : 51.54785716748076 W\n",
            "[codecarbon INFO @ 07:51:48] Energy consumed for all CPUs : 0.001190 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 07:51:48] 0.002751 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20, Loss: 2.196936070919037\n",
            "Training complete.\n",
            "Emissions: 0.001245372463993936 kg\n"
          ]
        }
      ],
      "source": [
        "# Energy Consumption from inference using CodeCarbon\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "# Specify device (CPU or GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move the model to the selected device\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# code carbon tracker\n",
        "tracker_code_carbon = EmissionsTracker()\n",
        "tracker_code_carbon.start()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for (inputs, labels) in data_loader:\n",
        "\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels[0]\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(data_loader)}')\n",
        "\n",
        "emissions_code_carbon = tracker_code_carbon.stop()\n",
        "\n",
        "print('Training complete.')\n",
        "print(f\"Emissions: {emissions_code_carbon} kg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwXCv4u-mB6c"
      },
      "source": [
        "## Use of [Carbontracker](https://github.com/lfwa/carbontracker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y0I8M4Lt2dw",
        "outputId": "57f540ed-954c-4804-fdf0-62c5773972c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CarbonTracker: The following components were found: GPU with device(s) Tesla T4.\n",
            "CarbonTracker: The following components were found: GPU with device(s) Tesla T4.\n",
            "CarbonTracker: WARNING - Failed to retrieve carbon intensity: Defaulting to average carbon intensity 357.2021 gCO2/kWh.\n",
            "CarbonTracker: WARNING - Failed to retrieve carbon intensity: Defaulting to average carbon intensity 357.2021 gCO2/kWh.\n",
            "CarbonTracker: WARNING - Failed to retrieve carbon intensity: Defaulting to average carbon intensity 357.2021 gCO2/kWh.\n",
            "CarbonTracker: WARNING - Failed to retrieve carbon intensity: Defaulting to average carbon intensity 357.2021 gCO2/kWh.\n",
            "CarbonTracker: Average carbon intensity during training was 357.20 gCO2/kWh at detected location: Council Bluffs, Iowa, US.\n",
            "CarbonTracker: Average carbon intensity during training was 357.20 gCO2/kWh at detected location: Council Bluffs, Iowa, US.\n",
            "CarbonTracker: \n",
            "Actual consumption for 1 epoch(s):\n",
            "\tTime:\t0:00:05\n",
            "\tEnergy:\t0.000108883678 kWh\n",
            "\tCO2eq:\t0.038893478597 g\n",
            "\tThis is equivalent to:\n",
            "\t0.000361799801 km travelled by car\n",
            "CarbonTracker: \n",
            "Actual consumption for 1 epoch(s):\n",
            "\tTime:\t0:00:05\n",
            "\tEnergy:\t0.000108883678 kWh\n",
            "\tCO2eq:\t0.038893478597 g\n",
            "\tThis is equivalent to:\n",
            "\t0.000361799801 km travelled by car\n",
            "CarbonTracker: WARNING - Failed to retrieve carbon intensity: Defaulting to average carbon intensity 357.2021 gCO2/kWh.\n",
            "CarbonTracker: WARNING - Failed to retrieve carbon intensity: Defaulting to average carbon intensity 357.2021 gCO2/kWh.\n",
            "CarbonTracker: Live carbon intensity could not be fetched at detected location: Council Bluffs, Iowa, US. Defaulted to average carbon intensity for US in 2021 of 357.20 gCO2/kWh. at detected location: Council Bluffs, Iowa, US.\n",
            "CarbonTracker: Live carbon intensity could not be fetched at detected location: Council Bluffs, Iowa, US. Defaulted to average carbon intensity for US in 2021 of 357.20 gCO2/kWh. at detected location: Council Bluffs, Iowa, US.\n",
            "CarbonTracker: \n",
            "Predicted consumption for 20 epoch(s):\n",
            "\tTime:\t0:01:50\n",
            "\tEnergy:\t0.002177673569 kWh\n",
            "\tCO2eq:\t0.777869571930 g\n",
            "\tThis is equivalent to:\n",
            "\t0.007235996018 km travelled by car\n",
            "CarbonTracker: \n",
            "Predicted consumption for 20 epoch(s):\n",
            "\tTime:\t0:01:50\n",
            "\tEnergy:\t0.002177673569 kWh\n",
            "\tCO2eq:\t0.777869571930 g\n",
            "\tThis is equivalent to:\n",
            "\t0.007235996018 km travelled by car\n",
            "CarbonTracker: Finished monitoring.\n",
            "CarbonTracker: Finished monitoring.\n",
            "Epoch 20/20, Loss: 1.887573778629303\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "# Energy Consumption from inference using CarbonTracker\n",
        "from carbontracker.tracker import CarbonTracker\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "# Specify device (CPU or GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move the model to the selected device\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# code carbon tracker\n",
        "tracker = CarbonTracker(epochs=num_epochs)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    tracker.epoch_start()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for (inputs, labels) in data_loader:\n",
        "\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels[0]\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "    tracker.epoch_end()\n",
        "\n",
        "tracker.stop()\n",
        "\n",
        "print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(data_loader)}')\n",
        "\n",
        "print('Training complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92SEhy29AmjG"
      },
      "source": [
        "## Use [PyJoules](https://github.com/powerapi-ng/pyJoules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBcI-6HIC-b3"
      },
      "outputs": [],
      "source": [
        "from pyJoules.energy_meter import measure_energy\n",
        "from pyJoules.handler.csv_handler import CSVHandler\n",
        "\n",
        "@measure_energy()\n",
        "def train_loop(num_epochs, data_loader, device, optimizer, criterrion, loss):\n",
        "  for (inputs, labels) in data_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels[0]\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)  # Add channel dimension\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHmMXPWrBoCC",
        "outputId": "c5ab14b0-049e-4ecd-8a1e-b24a421ba444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "begin timestamp : 1695196515.2832465; tag : train_loop; duration : 3.9616856575012207; nvidia_gpu_0 : 208155\n",
            "Epoch 1/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196519.2744637; tag : train_loop; duration : 4.41056752204895; nvidia_gpu_0 : 226366\n",
            "Epoch 2/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196523.7058737; tag : train_loop; duration : 4.466010332107544; nvidia_gpu_0 : 224584\n",
            "Epoch 3/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196528.2014816; tag : train_loop; duration : 3.786639451980591; nvidia_gpu_0 : 205291\n",
            "Epoch 4/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196532.0168674; tag : train_loop; duration : 3.7673113346099854; nvidia_gpu_0 : 204570\n",
            "Epoch 5/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196535.8147736; tag : train_loop; duration : 5.019988775253296; nvidia_gpu_0 : 253521\n",
            "Epoch 6/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196540.8559875; tag : train_loop; duration : 3.8286290168762207; nvidia_gpu_0 : 204288\n",
            "Epoch 7/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196544.716007; tag : train_loop; duration : 3.7648584842681885; nvidia_gpu_0 : 205016\n",
            "Epoch 8/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196548.5098677; tag : train_loop; duration : 4.290334224700928; nvidia_gpu_0 : 227252\n",
            "Epoch 9/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196552.821764; tag : train_loop; duration : 4.806065559387207; nvidia_gpu_0 : 243028\n",
            "Epoch 10/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196557.6565008; tag : train_loop; duration : 3.734696626663208; nvidia_gpu_0 : 199586\n",
            "Epoch 11/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196561.4190779; tag : train_loop; duration : 3.7484781742095947; nvidia_gpu_0 : 198692\n",
            "Epoch 12/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196565.1985075; tag : train_loop; duration : 4.721417427062988; nvidia_gpu_0 : 245017\n",
            "Epoch 13/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196569.940897; tag : train_loop; duration : 4.180655002593994; nvidia_gpu_0 : 214000\n",
            "Epoch 14/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196574.1498516; tag : train_loop; duration : 3.6961076259613037; nvidia_gpu_0 : 194293\n",
            "Epoch 15/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196577.8727434; tag : train_loop; duration : 3.808570146560669; nvidia_gpu_0 : 199417\n",
            "Epoch 16/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196581.7024837; tag : train_loop; duration : 5.098294258117676; nvidia_gpu_0 : 262812\n",
            "Epoch 17/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196586.831511; tag : train_loop; duration : 3.737574815750122; nvidia_gpu_0 : 206005\n",
            "Epoch 18/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196590.5995486; tag : train_loop; duration : 3.756683588027954; nvidia_gpu_0 : 199544\n",
            "Epoch 19/20, Loss: 0.41296249628067017\n",
            "begin timestamp : 1695196594.3864875; tag : train_loop; duration : 4.728349447250366; nvidia_gpu_0 : 245746\n",
            "Epoch 20/20, Loss: 0.41296249628067017\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "# Define the CRNN model\n",
        "model = CRNN(num_classes=10)\n",
        "\n",
        "# Move the model to the selected device\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    train_loop(num_epochs, data_loader, device, optimizer, criterion, loss)\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(data_loader)}')\n",
        "\n",
        "print('Training complete.')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}